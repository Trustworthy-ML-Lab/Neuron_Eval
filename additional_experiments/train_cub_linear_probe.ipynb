{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import data_utils\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.get_data(\"cub_train\", clip_preprocess)\n",
    "val_data = data_utils.get_data(\"cub_val\", clip_preprocess)\n",
    "test_data = data_utils.get_data(\"cub_test\", clip_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_image_features(model, dataset, batch_size=256, device = \"cuda\"):\n",
    "    #_make_save_dir(save_name)\n",
    "    all_features = []\n",
    "    all_concepts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, concept_labels in tqdm(DataLoader(dataset, batch_size)):\n",
    "            features = model.encode_image(images.to(device).float())\n",
    "            all_features.append(features)\n",
    "            all_concepts.append(torch.stack(concept_labels, dim=1).float().to(device))\n",
    "    return torch.cat(all_features),  torch.cat(all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:20<00:00,  1.88it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n",
      "100%|██████████| 46/46 [00:24<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feats, train_c = get_clip_image_features(clip_model, train_data, batch_size, device)\n",
    "val_feats, val_c = get_clip_image_features(clip_model, val_data, batch_size, device)\n",
    "test_feats, test_c = get_clip_image_features(clip_model, test_data, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2400], Loss: 0.3117\n",
      "Val Accuracy: 0.8675\n",
      "Epoch [200/2400], Loss: 0.2716\n",
      "Val Accuracy: 0.8823\n",
      "Epoch [300/2400], Loss: 0.2533\n",
      "Val Accuracy: 0.8879\n",
      "Epoch [400/2400], Loss: 0.2422\n",
      "Val Accuracy: 0.8909\n",
      "Epoch [500/2400], Loss: 0.2344\n",
      "Val Accuracy: 0.8932\n",
      "Epoch [600/2400], Loss: 0.2284\n",
      "Val Accuracy: 0.8946\n",
      "Epoch [700/2400], Loss: 0.2236\n",
      "Val Accuracy: 0.8956\n",
      "Epoch [800/2400], Loss: 0.2194\n",
      "Val Accuracy: 0.8961\n",
      "Epoch [900/2400], Loss: 0.2158\n",
      "Val Accuracy: 0.8972\n",
      "Epoch [1000/2400], Loss: 0.2127\n",
      "Val Accuracy: 0.8976\n",
      "Epoch [1100/2400], Loss: 0.2098\n",
      "Val Accuracy: 0.8977\n",
      "Epoch [1200/2400], Loss: 0.2072\n",
      "Val Accuracy: 0.8982\n",
      "Epoch [1300/2400], Loss: 0.2048\n",
      "Val Accuracy: 0.8986\n",
      "Epoch [1400/2400], Loss: 0.2026\n",
      "Val Accuracy: 0.8990\n",
      "Epoch [1500/2400], Loss: 0.2005\n",
      "Val Accuracy: 0.8992\n",
      "Epoch [1600/2400], Loss: 0.1985\n",
      "Val Accuracy: 0.8993\n",
      "Epoch [1700/2400], Loss: 0.1967\n",
      "Val Accuracy: 0.8995\n",
      "Epoch [1800/2400], Loss: 0.1950\n",
      "Val Accuracy: 0.8995\n",
      "Epoch [1900/2400], Loss: 0.1934\n",
      "Val Accuracy: 0.8995\n",
      "Epoch [2000/2400], Loss: 0.1918\n",
      "Val Accuracy: 0.8997\n",
      "Epoch [2100/2400], Loss: 0.1903\n",
      "Val Accuracy: 0.8998\n",
      "Epoch [2200/2400], Loss: 0.1889\n",
      "Val Accuracy: 0.8999\n",
      "Epoch [2300/2400], Loss: 0.1875\n",
      "Val Accuracy: 0.8999\n",
      "Epoch [2400/2400], Loss: 0.1862\n",
      "Val Accuracy: 0.8999\n",
      "Test Accuracy: 0.9017\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Logistic Regression Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, n_concepts):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, n_concepts)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = train_feats.shape[1]\n",
    "n_concepts = train_c.shape[1]\n",
    "model = LogisticRegression(input_dim, n_concepts).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2400#2400\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(train_feats)#/train_feats.norm(dim=1, keepdim=True))\n",
    "    loss = criterion(outputs, train_c.float())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(val_feats)#/val_feats.norm(dim=1, keepdim=True))\n",
    "            predictions = (predictions >= 0.5).float()\n",
    "            accuracy = (predictions == val_c).float().mean()\n",
    "            print(f\"Val Accuracy: {accuracy.item():.4f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_feats)#/test_feats.norm(dim=1, keepdim=True))\n",
    "    bin_predictions = (predictions >= 0.5).float()\n",
    "    accuracy = (bin_predictions == test_c).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.linear.weight)\n",
    "#torch.save(predictions, \"data/cub/clip_vit_b_32_probe_c_preds.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.linear, \"data/cub_linear_probe.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
